{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwOYkj9Si3HS"
   },
   "source": [
    "# 1. 데이터 불러오기\n",
    "- y값: g191a297 (A52. <취업자 공통> <이직 준비 관련> 이직준비를 하고 있는지 여부)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XIGS9EXkNJOh"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import semopy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from semopy import Model, report\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pydot\n",
    "from IPython.display import Image, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import ADASYN, BorderlineSMOTE, SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rD6tLGdwg0t"
   },
   "source": [
    "# 7. 모델링 - 예측 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr6kz2HiGMx7"
   },
   "source": [
    "## 7-1. SMOTE + 구조방정식 변수 20-> 23개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#현재 폴더 경로; 작업 폴더 기준\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-qJZPvDpiJd"
   },
   "outputs": [],
   "source": [
    "# clean_done_df 불러오기\n",
    "clean_done_df = pd.read_csv(f'{base_path}/data/clean_done_df.csv', encoding='EUC-KR', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done_df = clean_done_df.drop(columns= ['Y191ear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimates\n",
    "estimates = pd.read_csv(f'{base_path}/data/sem_model_estimates.csv',encoding='EUC-KR',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncjyYpSEEA_t"
   },
   "outputs": [],
   "source": [
    "# params 데이터프레임이 이미 있는 상태에서 진행합니다.\n",
    "params = estimates\n",
    "\n",
    "# 중요 변수 상위 20개 필터링 (요인 적재값 기준)\n",
    "important_params = params.loc[params['op'].isin(['~', '=~'])].copy()\n",
    "important_params['abs_estimate'] = important_params['Estimate'].abs()\n",
    "top_20_params = important_params.nlargest(23, 'abs_estimate') # 변수 개수 선택\n",
    "\n",
    "# 상위 20개의 변수명 추출\n",
    "top_20_vars = top_20_params['lval'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "floRVI5SEyMx",
    "outputId": "de93963c-d4a8-4f5f-c3b8-8916cd43d324",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_counts = clean_done_df['g191a297'].value_counts()\n",
    "print(y_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE 불균형 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_done_df 불러오기\n",
    "clean_done_df = pd.read_csv(f'{base_path}/data/clean_done_df.csv', encoding='EUC-KR', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_top_20 = clean_done_df[top_20_vars]\n",
    "y = clean_done_df['g191a297']  # 타겟 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE 적용\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_top_20, y)\n",
    "\n",
    "# resampled 데이터프레임 생성\n",
    "resampled_df = pd.DataFrame(X_resampled, columns=X_resampled.columns)\n",
    "resampled_df['g191a297'] = y_resampled\n",
    "\n",
    "# 클래스 0과 1의 개수 출력\n",
    "class_counts = resampled_df['g191a297'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled = X_resampled.drop(columns= ['g191a297'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': xgb.XGBClassifier(),\n",
    "    'LightGBM': lgb.LGBMClassifier()\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/Tree-based_model_results_SMOTE_23.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"Tree-based (SMOTE): Results have been saved to 'Tree-based_model_results_SMOTE_23.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data is in the correct format and contiguous in memory\n",
    "X_top_20 = np.ascontiguousarray(X_top_20, dtype=np.float64)\n",
    "y = np.ascontiguousarray(y, dtype=np.float64)\n",
    "\n",
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/SVM.KNNL_model_results_SMOTE_23.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"SVM.KNN (SMOTE): Results have been saved to 'SVM.KNNL_model_results_SMOTE_23.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/LR.NB_model_results_SMOTE_23.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"LR.NB (SMOTE): Results have been saved to 'LR.NB_model_results_SMOTE_23.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 베이지안 옵티마이제이션\n",
    "def optimize_model(model, param_bounds):\n",
    "    def model_evaluate(**params):\n",
    "        for param in params:\n",
    "            if param in ['max_depth', 'min_samples_split', 'n_estimators', 'num_leaves', 'n_neighbors', 'leaf_size']:\n",
    "                params[param] = int(params[param])\n",
    "        model.set_params(**params)\n",
    "        return cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5).mean()\n",
    "\n",
    "    optimizer = BayesianOptimization(f=model_evaluate, pbounds=param_bounds, random_state=42)\n",
    "    optimizer.maximize(init_points=10, n_iter=20)\n",
    "    \n",
    "    return optimizer.max['params']\n",
    "\n",
    "# 모델 리스트와 파라미터 범위\n",
    "models = {\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {\n",
    "        'max_depth': (1, 1000),\n",
    "        'min_samples_split': (2, 500) \n",
    "    }),\n",
    "    'Random Forest': (RandomForestClassifier(), {\n",
    "        'n_estimators': (10, 2000),\n",
    "        'max_depth': (1, 1000),\n",
    "        'min_samples_split': (2, 500)\n",
    "    }),\n",
    "    'XGBoost': (xgb.XGBClassifier(), {\n",
    "        'n_estimators': (10, 2000),\n",
    "        'max_depth': (1, 200), \n",
    "        'learning_rate': (0.001, 0.5)\n",
    "    }),\n",
    "    'LightGBM': (lgb.LGBMClassifier(), {\n",
    "        'n_estimators': (10, 2000), \n",
    "        'num_leaves': (20, 1000),\n",
    "        'learning_rate': (0.001, 0.5) \n",
    "    }),\n",
    "    'KNN': (KNeighborsClassifier(), {\n",
    "        'n_neighbors': (1, 500), \n",
    "        'leaf_size': (10, 1000) \n",
    "    }),\n",
    "    'Logistic Regression': (LogisticRegression(), {\n",
    "        'C': (0.001, 1000),\n",
    "        \n",
    "    }),\n",
    "    'Naive Bayes': (GaussianNB(), {\n",
    "        'var_smoothing': (1e-12, 1e-6)\n",
    "    })\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, (model, param_bounds) in models.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    best_params = optimize_model(model, param_bounds)\n",
    "    # 정수형 파라미터를 다시 정수로 변환\n",
    "    for param in best_params:\n",
    "        if param in ['max_depth', 'min_samples_split', 'n_estimators', 'num_leaves', 'n_neighbors', 'leaf_size']:\n",
    "            best_params[param] = int(best_params[param])\n",
    "    model.set_params(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\nBest Params: {best_params}\\n')\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/model_results_SMOTE_23_optimized.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"All models (SMOTE): Results have been saved to 'model_results_SMOTE_23_optimized.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 베이지안 옵티마이제이션\n",
    "def optimize_model(model, param_bounds):\n",
    "    def model_evaluate(**params):\n",
    "        for param in params:\n",
    "            if param in ['max_depth', 'min_samples_split', 'n_estimators', 'num_leaves', 'n_neighbors', 'leaf_size']:\n",
    "                params[param] = int(params[param])\n",
    "        model.set_params(**params)\n",
    "        return cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5).mean()\n",
    "\n",
    "    optimizer = BayesianOptimization(f=model_evaluate, pbounds=param_bounds, random_state=42)\n",
    "    optimizer.maximize(init_points=10, n_iter=20)\n",
    "    \n",
    "    return optimizer.max['params']\n",
    "\n",
    "# 모델 리스트와 파라미터 범위\n",
    "models = {\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {\n",
    "        'max_depth': (1, 1000),\n",
    "        'min_samples_split': (2, 500) \n",
    "    }),\n",
    "    'Random Forest': (RandomForestClassifier(), {\n",
    "        'n_estimators': (10, 2000),\n",
    "        'max_depth': (1, 1000),\n",
    "        'min_samples_split': (2, 500)\n",
    "    }),\n",
    "    'XGBoost': (xgb.XGBClassifier(), {\n",
    "        'n_estimators': (10, 2000),\n",
    "        'max_depth': (1, 200), \n",
    "        'learning_rate': (0.001, 0.5)\n",
    "    }),\n",
    "    'LightGBM': (lgb.LGBMClassifier(), {\n",
    "        'n_estimators': (10, 2000), \n",
    "        'num_leaves': (20, 1000),\n",
    "        'learning_rate': (0.001, 0.5) \n",
    "    }),\n",
    "    'KNN': (KNeighborsClassifier(), {\n",
    "        'n_neighbors': (1, 500), \n",
    "        'leaf_size': (10, 1000) \n",
    "    }),\n",
    "    'Logistic Regression': (LogisticRegression(), {\n",
    "        'C': (0.001, 1000),\n",
    "        \n",
    "    }),\n",
    "    'Naive Bayes': (GaussianNB(), {\n",
    "        'var_smoothing': (1e-12, 1e-6)\n",
    "    })\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, (model, param_bounds) in models.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    best_params = optimize_model(model, param_bounds)\n",
    "    # 정수형 파라미터를 다시 정수로 변환\n",
    "    for param in best_params:\n",
    "        if param in ['max_depth', 'min_samples_split', 'n_estimators', 'num_leaves', 'n_neighbors', 'leaf_size']:\n",
    "            best_params[param] = int(best_params[param])\n",
    "    model.set_params(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\nBest Params: {best_params}\\n')\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/model_results_SMOTE_23_optimized.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"All models (SMOTE): Results have been saved to 'model_results_SMOTE_23_optimized.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot and save SHAP values\n",
    "def plot_shap(model, X_train, model_name):\n",
    "    try:\n",
    "        if isinstance(model, (DecisionTreeClassifier, RandomForestClassifier, xgb.XGBClassifier, lgb.LGBMClassifier)):\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "        else:\n",
    "            explainer = shap.Explainer(model, X_train)\n",
    "        shap_values = explainer(X_train)\n",
    "        shap.summary_plot(shap_values, X_train, show=False)\n",
    "        plt.title(f\"SHAP Summary Plot for {model_name}\")\n",
    "        plt.savefig(f\"result/SHAP_{model_name}.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create SHAP plot for {model_name}: {str(e)}\")\n",
    "\n",
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "tree_based_models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': xgb.XGBClassifier(),\n",
    "    'LightGBM': lgb.LGBMClassifier()\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in tree_based_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "    plot_shap(model, X_train, model_name)\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/Tree-based_SHAP_results_SMOTE_23.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"Tree-based (SMOTE): Results have been saved to 'Tree-based_model_results_SMOTE_23.txt'.\")\n",
    "\n",
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "knn_model = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in knn_model.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "    plot_shap(model, X_train, model_name)\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/SVM.KNNL_SHAP_results_SMOTE_23.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"SVM.KNN (SMOTE): Results have been saved to 'SVM.KNNL_model_results_SMOTE_23.txt'.\")\n",
    "\n",
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "linear_models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in linear_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "    plot_shap(model, X_train, model_name)\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/LR.NBS_SHAP_results_SMOTE_23.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"LR.NB (SMOTE): Results have been saved to 'LR.NB_model_results_SMOTE_23.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(model, X, y, model_name, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X, y, train_sizes=train_sizes, cv=None, n_jobs=-1, scoring='accuracy'\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(f\"Learning Curve for {model_name}\")\n",
    "    plt.xlabel(\"Training steps\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Test score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(f\"result/Learning_Curve_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "tree_based_models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': xgb.XGBClassifier(),\n",
    "    'LightGBM': lgb.LGBMClassifier()\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in tree_based_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "    plot_shap(model, X_train, model_name)\n",
    "    plot_learning_curve(model, X_train, y_train, model_name)\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/Tree-based_Curve_results_SMOTE_23.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"Tree-based (SMOTE): Results have been saved to 'Tree-based_SHAP_results_SMOTE_23.txt'.\")\n",
    "\n",
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "knn_model = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in knn_model.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "    plot_shap(model, X_train, model_name)\n",
    "    plot_learning_curve(model, X_train, y_train, model_name)\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/SVM.KNNL_Curve_results_SMOTE_23.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"SVM.KNN (SMOTE): Results have been saved to 'SVM.KNNL_SHAP_results_SMOTE_23.txt'.\")\n",
    "\n",
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "linear_models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in linear_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "    plot_shap(model, X_train, model_name)\n",
    "    plot_learning_curve(model, X_train, y_train, model_name)\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/LR.NBS_Curve_results_SMOTE_23.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"LR.NB (SMOTE): Results have been saved to 'LR.NB_SHAP_results_SMOTE_23.txt'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 변수+ SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_done_df 불러오기\n",
    "clean_done_df = pd.read_csv(f'{base_path}/data/clean_done_df.csv', encoding='EUC-KR', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_done_df.drop(columns=['g191a297', 'Y191ear'])\n",
    "y = clean_done_df['g191a297']  # 타겟 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE 적용\n",
    "smote = SMOTE()\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "#데이터프레임 생성\n",
    "df = pd.DataFrame(X, columns=X.columns)\n",
    "df['g191a297'] = y\n",
    "\n",
    "# 클래스 0과 1의 개수 출력\n",
    "class_counts = df['g191a297'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns= ['g191a297'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': xgb.XGBClassifier(),\n",
    "    'LightGBM': lgb.LGBMClassifier()\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in models.items():\n",
    "    # 교차 검증을 통한 모델 평가\n",
    "    cross_val_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    results.append(f'{model_name} Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\\n')\n",
    "    results.append(f'Cross Validation Scores: {cross_val_scores}\\n')\n",
    "    results.append(f'Classification Report:\\n{report}\\n')\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/Training.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    plot_learning_curve(model, f'Learning Curve for {model_name}', X, y, cv=5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data is in the correct format and contiguous in memory\n",
    "X = np.ascontiguousarray(X, dtype=np.float64)\n",
    "y = np.ascontiguousarray(y, dtype=np.float64)\n",
    "\n",
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "models = {\n",
    "    'SVM': SVC(probability=True),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in models.items():\n",
    "    # 교차 검증을 통한 모델 평가\n",
    "    cross_val_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    results.append(f'{model_name} Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\\n')\n",
    "    results.append(f'Cross Validation Scores: {cross_val_scores}\\n')\n",
    "    results.append(f'Classification Report:\\n{report}\\n')\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/Training.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    plot_learning_curve(model, f'Learning Curve for {model_name}', X, y, cv=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "models = {\n",
    "\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': xgb.XGBClassifier(),\n",
    "    'LightGBM': lgb.LGBMClassifier()\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/Tree-based_model_results_SMOTE.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"Tree-based (SMOTE): Results have been saved to 'Tree-based_model_results_SMOTE.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data is in the correct format and contiguous in memory\n",
    "X = np.ascontiguousarray(X, dtype=np.float64)\n",
    "y = np.ascontiguousarray(y, dtype=np.float64)\n",
    "\n",
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/SVM.KNNL_model_results_SMOTE.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"KNN (SMOTE): Results have been saved to 'SVM.KNNL_model_results_SMOTE.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 리스트\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\n')\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/LR.NB_model_results_SMOTE.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"LR.NB (SMOTE): Results have been saved to 'LR.NB_model_results_SMOTE.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 베이지안 옵티마이제이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# 학습 및 테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 베이지안 옵티마이제이션을 위한 함수 정의\n",
    "def optimize_model(model, param_bounds):\n",
    "    def model_evaluate(**params):\n",
    "        for param in params:\n",
    "            if param in ['max_depth', 'min_samples_split', 'n_estimators', 'num_leaves', 'n_neighbors', 'leaf_size']:\n",
    "                params[param] = int(params[param])\n",
    "        model.set_params(**params)\n",
    "        return cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5).mean()\n",
    "\n",
    "    optimizer = BayesianOptimization(f=model_evaluate, pbounds=param_bounds, random_state=42)\n",
    "    optimizer.maximize(init_points=5, n_iter=10)\n",
    "    \n",
    "    return optimizer.max['params']\n",
    "\n",
    "\n",
    "# 모델 리스트와 파라미터 범위\n",
    "models = {\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {\n",
    "        'max_depth': (1, 1000),\n",
    "        'min_samples_split': (2, 500) \n",
    "    }),\n",
    "    'Random Forest': (RandomForestClassifier(), {\n",
    "        'n_estimators': (10, 2000),\n",
    "        'max_depth': (1, 1000),\n",
    "        'min_samples_split': (2, 500)\n",
    "    }),\n",
    "    'XGBoost': (xgb.XGBClassifier(), {\n",
    "        'n_estimators': (10, 2000),\n",
    "        'max_depth': (1, 200), \n",
    "        'learning_rate': (0.001, 0.5)\n",
    "    }),\n",
    "    'LightGBM': (lgb.LGBMClassifier(), {\n",
    "        'n_estimators': (10, 2000), \n",
    "        'num_leaves': (20, 1000),\n",
    "        'learning_rate': (0.001, 0.5) \n",
    "    }),\n",
    "    'KNN': (KNeighborsClassifier(), {\n",
    "        'n_neighbors': (1, 500), \n",
    "        'leaf_size': (10, 1000) \n",
    "    }),\n",
    "    'Logistic Regression': (LogisticRegression(), {\n",
    "        'C': (0.001, 1000),\n",
    "        \n",
    "    }),\n",
    "    'Naive Bayes': (GaussianNB(), {\n",
    "        'var_smoothing': (1e-12, 1e-6)\n",
    "    })\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가 결과 저장을 위한 파일\n",
    "results = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, (model, param_bounds) in models.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    best_params = optimize_model(model, param_bounds)\n",
    "    # 정수형 파라미터를 다시 정수로 변환\n",
    "    for param in best_params:\n",
    "        if param in ['max_depth', 'min_samples_split', 'n_estimators', 'num_leaves', 'n_neighbors', 'leaf_size']:\n",
    "            best_params[param] = int(best_params[param])\n",
    "    model.set_params(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    results.append(f'{model_name} Accuracy: {accuracy:.4f}\\n{report}\\nBest Params: {best_params}\\n')\n",
    "\n",
    "# 결과를 txt 파일로 저장\n",
    "with open('result/model_results_SMOTE_optimized.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"All models (SMOTE): Results have been saved to 'model_results_SMOTE_optimized.txt'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def plot_combined_learning_curve(models, X, y, model_names, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    for color, model_name, model in zip(colors, model_names, models):\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            model, X, y, scoring='accuracy'\n",
    "        )\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=color, label=f'{model_name} Test Accuracy')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Training Steps', fontsize=14)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example data, you should replace these with your actual data\n",
    "# X_resampled and y_resampled should be your data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# First group of models\n",
    "linear_and_knn_models = [\n",
    "    LogisticRegression(),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier()\n",
    "]\n",
    "\n",
    "linear_and_knn_model_names = [\n",
    "    'Logistic Regression',\n",
    "    'Naive Bayes',\n",
    "    'KNN',\n",
    "    'Decision Tree'\n",
    "]\n",
    "\n",
    "# Second group of models\n",
    "tree_based_models = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    xgb.XGBClassifier(),\n",
    "    lgb.LGBMClassifier()\n",
    "]\n",
    "\n",
    "tree_based_model_names = [\n",
    "    'Decision Tree',\n",
    "    'Random Forest',\n",
    "    'XGBoost',\n",
    "    'LightGBM'\n",
    "]\n",
    "\n",
    "# Plot learning curves for first group\n",
    "#plot_combined_learning_curve(linear_and_knn_models, X_train, y_train, linear_and_knn_model_names, 'Learning Curves for Logistic Regression, Naive Bayes, KNN, Decision Tree')\n",
    "\n",
    "# Plot learning curves for second group\n",
    "#plot_combined_learning_curve(tree_based_models, X_train, y_train, tree_based_model_names, 'Learning Curves for Decision Tree, Random Forest, XGBoost, LightGBM')\n",
    "\n",
    "# Function to plot SHAP values\n",
    "def plot_shap_values(model, X, model_name):\n",
    "    explainer = shap.Explainer(model, X)\n",
    "    shap_values = explainer(X)\n",
    "    shap.summary_plot(shap_values, X, plot_type=\"violin\", show=False)\n",
    "    plt.title(f'SHAP Summary Plot for {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "# Fit models and plot SHAP values\n",
    "for model, name in zip(linear_and_knn_models + tree_based_models, linear_and_knn_model_names + tree_based_model_names):\n",
    "    model.fit(X_train, y_train)\n",
    "    plot_shap_values(model, X_train, name)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "c4nsxcUval4f"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
